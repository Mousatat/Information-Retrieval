{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image search using [SIFT](https://www.thepythoncode.com/article/sift-feature-extraction-using-opencv-in-python)\n",
    "\n",
    "Let's think about information retrieval in the context of image search. How can we find images similar to a query in a fast way (faster than doing pair-wise comparison with all images in a database)? How can we identify same objects taken in slightly different contexts? \n",
    "\n",
    "One way to do this is to find special points of interest in every image, so called keypoints (or descriptors), which characterize the image and which are more or less invariant to scaling, orientation, illumination changes, and some other distortions. There are several algorithms available that identify such keypoints, and today we will focus on [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). \n",
    "\n",
    "Your task is to apply SIFT to a dataset of images and enable similar images search.\n",
    "\n",
    "## Get dataset\n",
    "\n",
    "We will use `Caltech 101` dataset, download it from [here](https://data.caltech.edu/records/mzrjq-6wc02). It consists of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT example\n",
    "\n",
    "Below is the example of SIFT keyponts extraction using `opencv`. [This](https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html) is a dedicated tutorial, and [this](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html) is another tutorial you may need to find matches between two images (use in your code `cv.drawMatches()` function to display keypoint matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python opencv-contrib-python\n",
    "# or use https://huggingface.co/datasets/will33am/Caltech101\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"will33am/Caltech101\")\n",
    "ds = load_dataset(\"will33am/Caltech101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = ds['test'], ds['train']\n",
    "\n",
    "CLASSES = sorted(list(set([row['label'] for row in train])))\n",
    "print(CLASSES)\n",
    "\n",
    "strawberries = [row for row in train if row['label'] == 'strawberry']\n",
    "wrenches = [row for row in train if row['label'] == 'wrench']\n",
    "wrenches[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# img_dir = '../../101_ObjectCategories'\n",
    "img = np.array(wrenches[17]['image'])\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# older versions of OpenCV\n",
    "# sift = cv.xfeatures2d.SIFT_create()\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "kp = sift.detect(gray, None)\n",
    "# use detectAndCompute(...) to get descriptors themselves\n",
    "\n",
    "print(f\"1st keypoint Location ({kp[0].pt[0]:.2f}, {kp[0].pt[1]:.2f})\")\n",
    "print(f\"1st keypoint Radius: {kp[0].size};  angle:{kp[0].angle}\")\n",
    "img=cv.drawKeypoints(gray, kp, img, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**Q**: Discuss what you see here. What is the meaning of circle diameter? Of the angle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of keypoints\n",
    "\n",
    "Let's suppose we've found image descriptors. How do we find similar images, having this information? In our case the descriptors are 128-dinensional vectors per keypoint, and there can be hundreds of such points. To enable fast search of similar images, you will index descriptors of all images using some data structure for approximate nearest neighbors search, such as Navigable Small World or Annoy. Then, for a (new) query image you will generate descriptors, and for each of these descriptors you will find its nearest neighbors (using Euclidean or Cosine distance, which you prefer). Finally, you will sort potential similar images (retrieved from neighbor descriptors) by frequency with which they appear in the nearest neighbors (more matches - higher the rank).\n",
    "\n",
    "### Build an index\n",
    "\n",
    "Read all images, saving category information. For every image generate SIFT descriptors and index them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all images and add their descriptors to index\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_sift_descriptors(img):    \n",
    "\n",
    "    # TODO return keypoints and their descriptors\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return kp, des\n",
    "        \n",
    "\n",
    "def get_top_descriptors(kp, des, top_k):\n",
    "    response_sort_indices = [i for (v, i) in sorted(((v, i) for (i, v) in enumerate(kp)), \n",
    "                                       key=lambda k: k[0].response, reverse=True)]        \n",
    "    top_des = np.take(des, response_sort_indices[:top_k], axis=0)\n",
    "    return top_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "kp, des = generate_sift_descriptors(wrenches[3]['image'])\n",
    "print(\"Keypoints:\", len(kp), len(des))\n",
    "print(\"Center:\", kp[4].pt)\n",
    "print(\"Vector:\\n\", des[4].reshape(16, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "dataset = train\n",
    "categories = list(set([row['label'] for row in dataset]))\n",
    "cat_lookup = dict((c, i) for i, c in enumerate(categories))\n",
    "\n",
    "for i, row in enumerate(tqdm(train)):\n",
    "    idn, img, label = row['id'], row['image'], row['label'] \n",
    "    keypoints, descriptors = generate_sift_descriptors(img)\n",
    "    vectors.append(get_top_descriptors(keypoints, descriptors, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "annoy = AnnoyIndex(128, 'euclidean')\n",
    "for i, vectors32 in enumerate(vectors):\n",
    "    for vec in vectors32:\n",
    "        annoy.add_item(i, vec)\n",
    "\n",
    "annoy.build(100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement search function\n",
    "\n",
    "Implement a function which returns `k` besr matching classes (names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def classifier(image, index, k):\n",
    "    keypoints, descriptors = generate_sift_descriptors(image)\n",
    "    vecs = get_top_descriptors(keypoints, descriptors, 32)\n",
    "    \n",
    "    # TODO:\n",
    "    # return the list of ordered pairs,\n",
    "    # contaning similarity and class name\n",
    "    \n",
    "    return counter.most_common(k)\n",
    "\n",
    "\n",
    "\n",
    "print(\"STRAWBERRY\")\n",
    "result = classifier(strawberries[22]['image'], annoy, 10)\n",
    "print(*result, sep='\\n')\n",
    "print()\n",
    "print(\"WRENCH\")\n",
    "result = classifier(wrenches[11]['image'], annoy, 10)\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Before going further, discuss, why SIFT is unsuitable for searching similar things. What is the application area? Is it reliable in this area, and to which extent?\n",
    "\n",
    "\n",
    "# 2. Deep classifiers and Embeddings\n",
    "\n",
    "Based on:\n",
    "- https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/\n",
    "- https://github.com/christiansafka/img2vec\n",
    "- https://github.com/ultralytics/yolov5\n",
    "\n",
    "### Obtain a single label for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Ferry_in_Istanbul_01.JPG/1200px-Ferry_in_Istanbul_01.JPG'\n",
    "im = imread(url)\n",
    "results = model(im)\n",
    "pandas_detections_df = results.pandas().xyxy[0]\n",
    "pandas_detections_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the classes for the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i, row in enumerate(dataset):\n",
    "    if i % 213 != 0: continue\n",
    "    results = model(row['image'])\n",
    "    tag = results.pandas().xyxy[0]['name']\n",
    "    tag = tag[0] if len(tag) else None\n",
    "    cat = row['label']\n",
    "    print(f\"{cat}\\t{tag}\")\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.imshow(np.array(row['image']))\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:** \n",
    "- Look at the results. \n",
    "- Can we use this for retrieval in the same way as we used SIFT features? \n",
    "- What if the labels are different from original? What if there are multiple or no labels?\n",
    "\n",
    "## Vector embedding for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install img2vec_pytorch Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image\n",
    "\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Ferry_in_Istanbul_01.JPG/1200px-Ferry_in_Istanbul_01.JPG'\n",
    "img = imread(url)\n",
    "\n",
    "# Initialize Img2Vec\n",
    "img2vec = Img2Vec(cuda=False)\n",
    "# some magic with broken Pillow.\n",
    "vector = img2vec.get_vec([Image.fromarray(img)]).reshape(-1)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = []\n",
    "\n",
    "def get_vectors(images):\n",
    "\n",
    "    # TODO\n",
    "    # return the np.array with the shape of (files x 512)\n",
    "\n",
    "    # some magic with broken pillow\n",
    "    # this should work: img2vec.get_vec([Image.fromarray(np.array(image))])\n",
    "    \n",
    "    return ...\n",
    "\n",
    "sorted_dataset = map(int, np.argsort([row['label'] for row in dataset]))\n",
    "images_sample = [dataset[row]['image'] for i, row in enumerate(sorted_dataset) if i < 200]\n",
    "embedding_vectors = get_vectors(images_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "d = pairwise_distances(embedding_vectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(d, cmap='RdBu', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
